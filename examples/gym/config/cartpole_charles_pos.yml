experiment: Cartpole
new_model: True
timestamp: True
experiment_config:
  agent: "RGRN"
  agent_kwargs:
    num_hidden: 6
    num_layers: 1
  world_kwargs:
    n_episodes: 4
    log_fields: [ "reward", "done", "observation"]  # need to log the observation for the position condition. this is done by the mindcraft framework
es: CHARLES
conditions:
  fitness_condition:                 # [Optional: generate offspring conditional to high fitness, comment if not desired]
    scale: 500                       #            normalize the fitness by this value
    greedy: true                     #            use greedy or Fisher-type sampling
  position_condition:                # <- that's the positional condition, see
    target: 0.5                      #    that's the target resting position of the cart pole
    horizon: 25                      #    the time horizon
    agg_horizon: mean                #    how to aggregate cart position information along the horizon (eg average resting position)
    agg_episode: mean                #    how to aggregated resting positions (pre-aggregated over horizon) across episodes
generations: 100
es_config:
  popsize: 256
  is_genetic_algorithm: False
  selection_pressure: 8.0            # selection pressure for roulette wheel selection
  adaptive_selection_pressure: False # whether to adapt the selection pressure
  elite_ratio: 0.25                  # ratio of the population to keep as elite. If crossover is used, an additional copy of the elite solutions might be added to the population if `improvement_steps` is defined.
  crossover_ratio: 0.4               # ratio of the population to perform crossover (if genetic algorithm, otherwise ignored)
  mutation_rate: 0.2                 # mutation rate after crossover and sampling
  unbiased_mutation_ratio: 0.1       # ratio of the population to perform unbiased mutation (if genetic algorithm, otherwise ignored)
  readaptation: 5                    # whether to use mutation adaptation (denoising steps) or not. Only valid for `HADES` and `CHARLES` evolutionary strategies.
  forget_best: True                  # whether to forget the best solution, or keep the elites for the next generation
  weight_decay: 0.001                # L2 weight decay for the parameters of the evolutionary strategy (NOT of the diffusion model)
  diff_optim: "Adam"                 # DM: optimizer for training the diffusion model
  diff_lr: 0.003                     # DM: learning rate for training the diffusion model
  diff_weight_decay: 0.0000001       # DM: weight decay for training the diffusion model
  diff_batch_size: 256               # DM: batch size for training the diffusion model
  diff_max_epoch:  100               # DM: number of training steps for the diffusion model at each generation
  diff_continuous_training: False    # DM: continuous training, or retrain diffusion model at each generation
  training_interval: 5               # DM: train the diffusion model every `training_interval` generations
  buffer_size:
    max_size: 10       # 0: no restriction to buffer-size, train on all data, otherwise, `max_size` specifies `max_size x posize` as maximum buffer size
    pop_type: quality  # diversity
diff: XPred            # pick from DDIM, RectFlow, VPred, XPred; Different models have different performance. XPred seems to work really good
diff_config:
  num_steps:   100     # number of denoising steps
  noise_level: 1.0     # noise level during stochastic denoising
  diff_range:    3     # +- parameter range for all parameters, limits search space
  alpha_schedule: cosine_nichol
#  scaler: StandardScaler  # optional: use a StandardScaler before training the DM on data, see condevo.preprocessing
nn: MLP
nn_config:
  num_hidden: 128
  num_layers: 2
  activation: ELU
  layer_norm: True
